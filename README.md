# Git Practice
## AI can predict which criminals may break laws again better than humans
I recently read an [article](https://www.sciencenews.org/article/ai-can-predict-criminals-repeat-offenders-better-than-humans) on _Science News_ about the use of AI to predict future criminal activity based on past data. I find the use of AI to predict human behavior absolutely fascinating and worthy of research, but when we start expecting to be able to make legal decisions based on the predictions made by such software it raises some big ethical questions about our relationship with technology. We've reached the age where AI and ML are becoming advanced enough to predict human behavior better than humans do, but as AI algorithms become more complex our understanding of how exactly they produce predictions diminishes. In my opinion, when it comes to upholding the law, AI based evidence definitely should be taken into account, but just like polygraph tests, which correctly detect that a person is lying only **87%** of the time (according to [this article](https://www.psychologytoday.com/us/blog/the-nature-deception/202001/do-lie-detector-tests-really-work) from *Psychology Today*), they should be taken with a grain of salt.

### Comments:
#### sdl433: 
This is really cool. This goes to show how poweful AI is becoming, and reminds me of the idea of AI and ML being used as a form of evidence in courts. That is, given the set of features associated with a suspect, if the conditional probability is sufficiently high (a threshold that's decided upon) then it can be used as evidence.


#### Comment from Zach Waxman: 
very interesting, thank you for sharing. We must be careful about applying technology to ethical questions.
